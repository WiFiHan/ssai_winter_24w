{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7b712b-abe8-4677-b006-001e8b1e9a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torchtext\n",
    "\n",
    "def visualize_mnist(dataloader, model=None, title=\"\"):\n",
    "    x, y = next(iter(dataloader_test))\n",
    "    fig, axes = plt.subplots(2, 5, layout=\"constrained\")\n",
    "    for i in range(10):\n",
    "        ax = axes[i//5, i%5]\n",
    "        for j in range(BATCH_SIZE):\n",
    "            if y[j].item() == i:\n",
    "                break\n",
    "#         plt.subplot(2, 5, i+1)\n",
    "        ax.imshow(x[j, 0], cmap='gray')\n",
    "        if model is not None:\n",
    "            y_hat = model(x[j:j+1].cuda()).argmax()\n",
    "            ax.set_title(f\"Prediction: {y_hat.item()}\")\n",
    "        else:\n",
    "            ax.set_title(f\"Answer: {y[j].item()}\")\n",
    "        ax.axis('off')\n",
    "    fig.tight_layout(pad=0.2, h_pad=-5)\n",
    "    fig.suptitle(title, y=0.93)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3c3e65-8ba9-496b-99b7-157c9c253d61",
   "metadata": {},
   "source": [
    "# Lab3-1. Image Classification Using FC Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6f2102",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 500\n",
    "LR = 0.001\n",
    "TOTAL_EPOCH = 10\n",
    "\n",
    "# Prepare MNIST datasets\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                        # 입력을 PyTorch tensor로 바꾸겠다\n",
    "    transforms.Normalize((0.1307,), (0.3081,))    # mean=0.1307, std=0.3081 로 normalize 하겠다\n",
    "])\n",
    "dataset_train = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "dataset_test = datasets.MNIST('../data', train=False, transform=transform)\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Visualize\n",
    "visualize_mnist(dataloader_test, title=\"MNIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1642df5-e04f-437a-ba91-de8bf07352b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [Batch_size, Channels, Height, Width] = [500, 1, 28, 28]\n",
    "        x = x.reshape(-1, 28*28) # x.shape: [Batch_size, 768]\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net(1024).cuda()  # Create a network (parameters are randomly initialized)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)  # Create an optimizer\n",
    "\n",
    "\n",
    "# Visualize model predictions before training\n",
    "visualize_mnist(dataloader_test, model, \"Model prediction before training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c548c14",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    iteration = 0\n",
    "    for x, target in dataloader:\n",
    "        iteration = iteration + 1\n",
    "        x, target = x.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        if iteration % 10 == 0:\n",
    "            print(\n",
    "                f'\\r[Training...] Epoch: {epoch}, '\n",
    "                f'Iteration: {iteration}/{len(dataloader)}, '\n",
    "                f'train_loss: {train_loss / iteration:.4f}',\n",
    "                flush=True,\n",
    "                end=\"\"\n",
    "            )\n",
    "    return train_loss / iteration\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, dataloader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    iteration = 0\n",
    "    for x, target in dataloader:\n",
    "        iteration = iteration + 1\n",
    "        x, target = x.cuda(), target.cuda()\n",
    "        output = model(x)\n",
    "        test_loss += F.cross_entropy(output, target).item()\n",
    "        pred = output.argmax(dim=1)  # get the index of the max log-probability\n",
    "        correct += (pred == target).sum().item()\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / len(dataloader.dataset)\n",
    "    return test_loss, accuracy\n",
    "\n",
    "\n",
    "# 학습 전의 loss 측정\n",
    "test_loss, test_accuracy = test(model, dataloader_test)\n",
    "print(f\"\\rBefore Training - \"\n",
    "      f\"test_loss: {test_loss:.4f}, test_accuracy: {test_accuracy:.1f}%\")\n",
    "\n",
    "\n",
    "# 학습 하면서 loss 측정\n",
    "for epoch in range(1, TOTAL_EPOCH+1):\n",
    "    train_loss = train(model, dataloader_train, optimizer, epoch)\n",
    "    test_loss, test_accuracy = test(model, dataloader_test)\n",
    "    print(f\"\\rEpoch {epoch} - train_loss: {train_loss:.4f}, \"\n",
    "          f\"test_loss: {test_loss:.4f}, test_accuracy: {test_accuracy:.1f}%\")\n",
    "\n",
    "\n",
    "# Visualize model predictions after training\n",
    "visualize_mnist(dataloader_test, model, \"Model prediction after training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10172fdf-506c-4ef7-a669-7679f6955e81",
   "metadata": {},
   "source": [
    "# Lab3-2. Image Classification Using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f61a01-6a79-4986-b8a9-c2e9cab88ee3",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 500\n",
    "LR = 0.001\n",
    "TOTAL_EPOCH = 10\n",
    "\n",
    "\n",
    "# Prepare MNIST datasets\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "dataset_train = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "dataset_test = datasets.MNIST('../data', train=False, transform=transform)\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "# Define a network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,  out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.avgpool = nn.AvgPool2d(2)\n",
    "        self.fc = nn.Linear(128*3*3, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape = [Batch_size, 1, 28, 28]\n",
    "        x = self.conv1(x)       # x.shape = [BATCH_SIZE, 32, 28, 28]\n",
    "        x = F.relu(x)\n",
    "        x = self.avgpool(x)     # x.shape = [BATCH_SIZE, 32, 14, 14]\n",
    "        x = self.conv2(x)       # x.shape = [BATCH_SIZE, 64, 14, 14]\n",
    "        x = F.relu(x)\n",
    "        x = self.avgpool(x)     # x.shape = [BATCH_SIZE, 64, 7, 7]\n",
    "        x = self.conv3(x)       # x.shape = [BATCH_SIZE, 128, 7, 7]\n",
    "        x = F.relu(x)\n",
    "        x = self.avgpool(x)     # x.shape = [BATCH_SIZE, 128, 3, 3]\n",
    "        x = torch.flatten(x, 1) # x.shape = [BATCH_SIZE, 128*3*3]\n",
    "        x = self.fc(x)         # x.shape = [BATCH_SIZE, 10]\n",
    "        # output = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net().cuda()  # Create a network (parameters are randomly initialized)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)  # Create an optimizer\n",
    "\n",
    "\n",
    "# Visualize model predictions before training\n",
    "visualize_mnist(dataloader_test, model, \"Model prediction before training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49229768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 하기 전의 loss 측정\n",
    "test_loss, test_accuracy = test(model, dataloader_test)\n",
    "print(f\"\\rBefore Training - \"\n",
    "      f\"test_loss: {test_loss:.4f}, test_accuracy: {test_accuracy:.1f}%\")\n",
    "\n",
    "\n",
    "# Train & validate 하면서 loss 측정\n",
    "for epoch in range(1, TOTAL_EPOCH+1):\n",
    "    train_loss = train(model, dataloader_train, optimizer, epoch)\n",
    "    test_loss, test_accuracy = test(model, dataloader_test)\n",
    "    print(f\"\\rEpoch {epoch} - train_loss: {train_loss:.4f}, \"\n",
    "          f\"test_loss: {test_loss:.4f}, test_accuracy: {test_accuracy:.1f}%\")\n",
    "\n",
    "\n",
    "# Visualize model predictions after training\n",
    "visualize_mnist(dataloader_test, model, \"Model prediction after training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3afa228-5f0d-41b6-b941-261e6a3ab1db",
   "metadata": {},
   "source": [
    "# Lab3-3. Speech Command Classification Using RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223c05e6-e2b6-4440-9f37-6d058f8c66b8",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare datasets\n",
    "class SubsetSC(SPEECHCOMMANDS):\n",
    "    def __init__(self, subset: str = None):\n",
    "        super().__init__(\"./\", download=True)\n",
    "\n",
    "        def load_list(filename):\n",
    "            filepath = os.path.join(self._path, filename)\n",
    "            with open(filepath) as fileobj:\n",
    "                return [os.path.normpath(os.path.join(self._path, line.strip())) for line in fileobj]\n",
    "\n",
    "        if subset == \"validation\":\n",
    "            self._walker = load_list(\"validation_list.txt\")\n",
    "        elif subset == \"testing\":\n",
    "            self._walker = load_list(\"testing_list.txt\")\n",
    "        elif subset == \"training\":\n",
    "            excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\")\n",
    "            excludes = set(excludes)\n",
    "            self._walker = [w for w in self._walker if w not in excludes]\n",
    "\n",
    "\n",
    "train_set = SubsetSC(\"training\")\n",
    "test_set = SubsetSC(\"testing\")\n",
    "\n",
    "waveform, sample_rate, label, speaker_id, utterance_number = train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb3e97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "waveform, sample_rate, label, speaker_id, utterance_number = train_set[1]\n",
    "plt.plot(waveform.t().numpy())\n",
    "plt.show()\n",
    "ipd.display(ipd.Audio(waveform.numpy(), rate=sample_rate))\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06900118",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sorted(list(set(datapoint[2] for datapoint in train_set)))\n",
    "\n",
    "def label_to_index(word):\n",
    "    # Return the position of the word in labels\n",
    "    return torch.tensor(labels.index(word))\n",
    "\n",
    "\n",
    "def index_to_label(index):\n",
    "    # Return the word corresponding to the index in labels\n",
    "    # This is the inverse of label_to_index\n",
    "    return labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590ab6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "\n",
    "\n",
    "def pad_sequence(batch):\n",
    "    # Make all tensor in a batch the same length by padding with zeros\n",
    "    batch = [item.t() for item in batch]\n",
    "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
    "    return batch.permute(0, 2, 1)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # A data tuple has the form:\n",
    "    # waveform, sample_rate, label, speaker_id, utterance_number\n",
    "    tensors, targets = [], []\n",
    "\n",
    "    # Gather in lists, and encode labels as indices\n",
    "    for waveform, _, label, *_ in batch:\n",
    "        tensors += [waveform]\n",
    "        targets += [label_to_index(label)]\n",
    "\n",
    "    # Group the list of tensors into a batched tensor\n",
    "    tensors = pad_sequence(tensors)\n",
    "    targets = torch.stack(targets)\n",
    "\n",
    "    return tensors, targets\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84508f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size=80, hidden_size=384, num_layers=4, output_size=35):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.1,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.output = nn.Linear(2*hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape: [BATCH_SIZE, 1, 80, Length]\n",
    "        x = x.squeeze(1).transpose(1, 2)  # x.shape: [BATCH_SIZE, LENGTH, 80]\n",
    "        _, (h, c) = self.lstm(x)          # h.shape: [2*num_layers, BATCH_SIZE, hidden_size]\n",
    "        x = h.transpose(0, 1)[:, -2:, :].reshape(-1, 2*self.hidden_size)  # [BATCH_SIZE, 2*hidden_size]\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "model = Net().cuda()\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate, n_fft=512, hop_length=256, n_mels=80).cuda()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "def train(model, epoch, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (x, target) in enumerate(train_loader):\n",
    "        x = x.cuda()\n",
    "        target = target.cuda()\n",
    "\n",
    "        # forward propagation\n",
    "        x = mel_spectrogram(x)\n",
    "        output = model(x)\n",
    "\n",
    "        # loss\n",
    "        loss = F.cross_entropy(output, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print training stats\n",
    "        print(\n",
    "            f\"\\rTrain Epoch: {epoch} [{batch_idx * len(x)}/{len(train_loader.dataset)} \"\n",
    "            f\"({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\",\n",
    "            flush=True,\n",
    "            end=\"\"\n",
    "        )\n",
    "\n",
    "        # record loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "\n",
    "def test(model, epoch):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for x, target in test_loader:\n",
    "        x = x.cuda()\n",
    "        target = target.cuda()\n",
    "\n",
    "        # forward propagation\n",
    "        x = mel_spectrogram(x)\n",
    "        output = model(x)\n",
    "\n",
    "        pred = output.argmax(dim=-1)\n",
    "        correct += (pred == target).sum().item()\n",
    "\n",
    "    print(f\"\\nTest Epoch: {epoch}\\tAccuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n\")\n",
    "\n",
    "\n",
    "TOTAL_EPOCH = 2\n",
    "losses = []\n",
    "\n",
    "# The transform needs to live on the same device as the model and the data.\n",
    "for epoch in range(1, TOTAL_EPOCH + 1):\n",
    "    train(model, epoch, log_interval)\n",
    "    test(model, epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
